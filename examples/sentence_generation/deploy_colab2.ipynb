{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generation-deploy-colab2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC5OwyKMx_l9"
      },
      "source": [
        "# 각종 설정\n",
        "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKybDwDqFIX5"
      },
      "source": [
        "from ratsnlp.nlpbook.generation import GenerationDeployArguments\n",
        "args = GenerationDeployArguments(\n",
        "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-generation\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mThtbxyNyO"
      },
      "source": [
        "# 모델 로딩\n",
        "프리트레인한(혹은 파인튜닝을 마친) GPT2 모델과 토크나이저를 읽어 들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFV031RZFRgD"
      },
      "source": [
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "if args.downstream_model_checkpoint_path is None:\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        args.pretrained_model_name,\n",
        "    )\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive', force_remount=True)\n",
        "    pretrained_model_config = GPT2Config.from_pretrained(\n",
        "        args.pretrained_model_name,\n",
        "    )\n",
        "    model = GPT2LMHeadModel(pretrained_model_config)\n",
        "    fine_tuned_model_ckpt = torch.load(\n",
        "        args.downstream_model_checkpoint_fpath,\n",
        "        map_location=torch.device(\"cpu\"),\n",
        "    )\n",
        "    model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3amlsjpFd9i"
      },
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    eos_token=\"</s>\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVsdmThyV_p"
      },
      "source": [
        "# 인퍼런스 함수 선언\n",
        "인퍼런스 함수를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnzR9NMtFiAz"
      },
      "source": [
        "def inference_fn(\n",
        "        prompt,\n",
        "        min_length=10,\n",
        "        max_length=20,\n",
        "        top_p=1.0,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.0,\n",
        "        no_repeat_ngram_size=0,\n",
        "        temperature=1.0,\n",
        "):\n",
        "    try:\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids,\n",
        "                do_sample=True,\n",
        "                top_p=float(top_p),\n",
        "                top_k=int(top_k),\n",
        "                min_length=int(min_length),\n",
        "                max_length=int(max_length),\n",
        "                repetition_penalty=float(repetition_penalty),\n",
        "                no_repeat_ngram_size=int(no_repeat_ngram_size),\n",
        "                temperature=float(temperature),\n",
        "           )\n",
        "        generated_sentence = tokenizer.decode([el.item() for el in generated_ids[0]])\n",
        "    except:\n",
        "        generated_sentence = \"\"\"처리 중 오류가 발생했습니다. <br>\n",
        "            변수의 입력 범위를 확인하세요. <br><br> \n",
        "            min_length: 1 이상의 정수 <br>\n",
        "            max_length: 1 이상의 정수 <br>\n",
        "            top-p: 0 이상 1 이하의 실수 <br>\n",
        "            top-k: 1 이상의 정수 <br>\n",
        "            repetition_penalty: 1 이상의 실수 <br>\n",
        "            no_repeat_ngram_size: 1 이상의 정수 <br>\n",
        "            temperature: 0 이상의 실수\n",
        "            \"\"\"\n",
        "    return {\n",
        "        'result': generated_sentence,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP6ZAaSybge"
      },
      "source": [
        "# 웹서비스 개시\n",
        "아래처럼 실행해 인퍼런스 함수를 웹서비스로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_up1ARoHFwLN"
      },
      "source": [
        "from ratsnlp.nlpbook.generation import get_web_service_app\n",
        "app = get_web_service_app(inference_fn)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}