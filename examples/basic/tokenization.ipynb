{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFt_JLYoTKA6"
      },
      "source": [
        "## 말뭉치 다운로드 및 전처리\n",
        "\n",
        "오픈소스 파이썬 패키지 코포라(Korpora)를 활용해 BPE 수행 대상 말뭉치를 내려받고 전처리합니다. 실습용 말뭉치는 박은정 님이 공개하신 Naver Sentiment Movie Corpus(NSMC)입니다.\n",
        "\n",
        "다음을 수행해 데이터를 내려받아 `nsmc`라는 변수로 읽어들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThW-2OrTHZW"
      },
      "source": [
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load(\"nsmc\", force_download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0NCnzpRTdnJ"
      },
      "source": [
        "다음을 수행하면 NSMC에 포함된 영화 리뷰(순수 텍스트)들을 지정된 경로에 저장해 둡니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSVDbnFUTiXx"
      },
      "source": [
        "import os\n",
        "def write_lines(path, lines):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n",
        "\n",
        "write_lines(\"/root/train.txt\", nsmc.train.get_all_texts())\n",
        "write_lines(\"/root/test.txt\", nsmc.test.get_all_texts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWboOauTTyXp"
      },
      "source": [
        "`train.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tup5LreLT4vE"
      },
      "source": [
        "!head /root/train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMKHuGgyT8aw"
      },
      "source": [
        "`test.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI5EpyyaT_fl"
      },
      "source": [
        "!head /root/test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2JYqRGUFll"
      },
      "source": [
        "## GPT 토크나이저 구축\n",
        "\n",
        "GPT 계열 모델이 사용하는 토크나이저는 Byte-level Byte Pair Encoding(BBPE)입니다. 다음을 수행하면 `nsmc` 데이터를 가지고 BBPE 어휘집합을 구축합니다. BBPE 어휘집합 구축에 시간이 걸리니 잠시 기다려주세요. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rk2Ga65USFb"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=[\"/root/train.txt\", \"/root/test.txt\"],\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\"[PAD]\"]\n",
        ")\n",
        "bytebpe_tokenizer.save_model(\"/root\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLq1JRVJUb7U"
      },
      "source": [
        "위의 코드 수행이 끝나면 `/root` 디렉토리에 `vocab.json`과 `merges.txt`가 생성됩니다. 전자는 바이트 레벨 BPE의 어휘 집합이며 후자는 바이그램 쌍의 병합 우선순위입니다. \n",
        "\n",
        "`vocab.json`은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZl8OKdsVIQg"
      },
      "source": [
        "!cat /root/vocab.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZsJ4SrVXnS"
      },
      "source": [
        "`merges.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzkJ31vQVbjl"
      },
      "source": [
        "!head /root/merges.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR-3V6zIVsPp"
      },
      "source": [
        "## BERT 토크나이저 구축\n",
        "\n",
        "BERT는 워드피스(wordpiece) 토크나이저를 사용합니다. 다음을 수행하면 BERT 모델이 사용하는 워드피스 어휘집합을 구축할 수 있습니다. 워드피스 어휘집합 구축에 시간이 걸리니 잠시만 기다려주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdvUPuJoV3w3"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=[\"/root/train.txt\", \"/root/train.txt\"],\n",
        "    vocab_size=10000,\n",
        ")\n",
        "wordpiece_tokenizer.save_model(\"/root\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0S2EbdkWIdq"
      },
      "source": [
        "위의 코드 수행이 끝나면 `/root` 디렉토리에 `vocab.txt`가 생성됩니다. `vocab.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWkywHaWOL6"
      },
      "source": [
        "!head /root/vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jsq7yrjWYTB"
      },
      "source": [
        "## GPT 입력값 만들기\n",
        "\n",
        "GPT 모델 입력값을 만들려면 Byte-level Byte Pair Encoding 어휘집합 구축 결과(`vocab.json`, `merges.txt`)가 필요합니다. 다음을 수행해 이미 만들어 놓은 BBPE 어휘집합을 포함한 GPT 토크나이저를 `tokenizer_gpt`라는 변수로 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJRVVpC-WyIc"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"/root\")\n",
        "tokenizer_gpt.pad_token = \"[PAD]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n3JlkUbZobu"
      },
      "source": [
        "예시 문장 세 개를 각각 토큰화해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp48wVBIZtYj"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ahPBg-Zx71"
      },
      "source": [
        "토큰화 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DWz4djZz_u"
      },
      "source": [
        "tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFQWvXsFXC1H"
      },
      "source": [
        "이번 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATwWfngCXQXK"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding=\"max_length\",\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9g0tJ9uXj6O"
      },
      "source": [
        "`batch_inputs`의 내용을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4pnC6DjXkv6"
      },
      "source": [
        "batch_inputs.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJBRdJegXsZd"
      },
      "source": [
        "batch_inputs['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_XT5QTsXvBG"
      },
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzWd3u-vXz9c"
      },
      "source": [
        "## BERT 입력값 만들기\n",
        "\n",
        "BERT 모델 입력값을 만들려면 워드피스 어휘집합 구축 결과(`vocab.txt`)가 필요합니다. 다음을 수행해 위에서 이미 만들어 놓은 워드피스 어휘집합을 포함한 BERT 토크나이저를 `tokenizer_bert`라는 변수로 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTFeKKzJX-O7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    \"/root\", \n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcxOXJxaER5"
      },
      "source": [
        "예시 문장 세 개를 각각 토큰화해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9AzWXcvaJKH"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJhOHq-maNLw"
      },
      "source": [
        "토큰화 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdzBTLFPaPFU"
      },
      "source": [
        "tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMxTUoowYHb2"
      },
      "source": [
        "이번 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_sFosQjYIE3"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding=\"max_length\",\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohNW5R3zYOW5"
      },
      "source": [
        "`batch_inputs`의 내용을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZiMSoaKYTUX"
      },
      "source": [
        "batch_inputs.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pWaGuSdYZGR"
      },
      "source": [
        "batch_inputs['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djtA4xkIYbOk"
      },
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVR15VXZYoTZ"
      },
      "source": [
        "batch_inputs['token_type_ids']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}