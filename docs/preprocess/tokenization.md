---
layout: default
title: Tokenization
parent: Preprocess
nav_order: 1
---

# Tokenization
{: .no_toc }

토큰화란 문장을 토큰 시퀀스로 분절하는 과정입니다. 토큰화 수행 대상에 따라 문자 단위, 단어 단위, 서브워드 단위 세 가지로 나뉩니다. 각 방법의 장단점 등을 살펴봅니다.
{: .fs-4 .ls-1 .code-example }

1. TOC
{:toc}

---

## 토큰화란

이 책에서 다루는 데이터의 기본 단위는 텍스트 형태의 문장(sentence)입니다. 토큰화(tokenization)란 문장을 토큰(token) 시퀀스로 분절하는 과정을 가리킵니다. 우리 책 튜토리얼에서 사용하는 트랜스포머 모델은 자연어 문장을 분절한 토큰 시퀀스를 입력으로 받습니다.

토큰화를 수행하는 프로그램을 토크나이저(tokenizer)라고 합니다. 대표적인 한국어 토크나이저로는 은전한닢(mecab), 꼬꼬마(kkma) 등이 있습니다. 이들 형태소 분석기는 언어 전문가들이 분절 및 태깅해 놓은 데이터를 학습해 최대한 전문가들이 분석해 놓은 결과와 비슷하게 토큰화를 수행합니다. 

이들 분석기는 토큰화뿐 아니라 품사 부착(Part-Of-Speech Tagging)까지 수행하기 때문에, 토큰화 개념을 광의로 해석하는 쪽에서는 토큰화를 토큰 분절 및 품사 부착까지 일컫는 경우도 종종 있습니다.


---

## 단어 단위 토큰화

토큰화 방식에는 여러 가지가 있습니다. 우선 단어(word) 단위로 토큰화를 수행할 수 있습니다. 가장 쉽게는 공백으로 분리할 수 있겠네요. 예컨대 다음과 같습니다.

## **그림1** 공백 단위 토큰화
{: .no_toc .text-delta } 
```
어제 카페 갔었어 > 어제, 카페, 갔었어
어제 카페 갔었는데요 > 어제, 카페, 갔었는데요
```

그림1처럼 토큰화를 수행하면 별도로 토크나이저를 쓰지 않아도 된다는 장점이 있지만 사용해야 하는 어휘 집합의 크기가 기하급수적으로 늘어날 수 있습니다. 그림1처럼 그 표현이 살짝 바뀐 경우에도 해당 표현 모두(`갔었어`, `갔었는데요` 등)가 어휘 집합에 포함되어 있어야 합니다.

학습된 토크나이저를 사용하면 이를 좀 완화할 수 있습니다. 한국어의 경우 은전한닢(mecab)이 대표적인 분석기입니다. 그림2는 같은 문장을 은전한닢으로 토큰화한 결과입니다. 예시의 수가 적어서 그 효과가 도드라져 보이지는 않지만 분절을 의미 있는 단위(`갔었`)로 수행해 어휘 집합 크기의 급격한 증가를 다소 막을 수 있습니다.

## **그림2** 은전한닢을 활용한 토큰화
{: .no_toc .text-delta } 
```
어제 카페 갔었어 > 어제, 카페, 갔었, 어
어제 카페 갔었는데요 > 어제, 카페, 갔었, 는데요
```

은전한닢 같은 토크나이저를 사용하더라도 어휘 집합 크기가 지나치게 커지는 건 막기 어렵습니다. 보통 언어 하나로 모델을 구축하려는 경우 어휘 집합 크기가 10만개를 훌쩍 넘어버리는 경우가 다반사입니다. 어휘 집합 크기가 커지면 우리가 구축할 모델의 사이즈가 지나치게 커진다는 말이 되고 이는 결국 메모리 문제를 야기하게 돼 학습 자체가 불가능해 질 수 있습니다.


---


## 문자 단위 토큰화

단어 단위 대신 문자(character) 단위 토큰화를 고려해볼 수 있습니다. 한국어 언어모델을 만든다고 하면, 한글 음절 수는 모두 1만1172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 어휘 집합 크기는 기껏해야 1만5000개를 넘기 어렵습니다. 

게다가 해당 언어의 모든 문자를 어휘 집합에 포함시켰기 때문에 UNK 문제로부터도 자유롭습니다. UNK란 어휘 집합에 없는 토큰을 가리키는데요. 주로 신조어 등에서 발생합니다. 

하지만 문자 단위로 토큰화를 수행할 경우(그림3) 단점이 분명 존재합니다. 우선 각 문자 토큰은 의미 있는 단위가 되기 어렵습니다. 예컨대 `어제`의 `어`와 어미 `어`의 구분이 사라진다든지 말이죠.

## **그림3** 문자 단위 토큰화
{: .no_toc .text-delta } 
```
어제 카페 갔었어 > 어, 제, 카, 페, 갔, 었, 어
어제 카페 갔었는데요 > 어, 제, 카, 페, 갔, 었, 는, 데, 요
```

뿐만 아니라 그림1, 그림2의 단어 단위 토큰화와 비교할 때 분석 결과인 토큰 시퀀스의 길이가 상대적으로 길어졌음을 확인할 수 있습니다. 모델 입력이 되는 토큰 시퀀스가 길면 모델이 해당 문장을 학습하기가 어려워지고 결과적으로 성능의 감소를 가져오게 됩니다.


---

## 서브워드 단위 토큰화

서브워드(subword) 단위 토큰화는 단어 단위와 문자 단위 토큰화의 중간에 있는 형태입니다. 둘의 장점만을 취한 형태입니다. 어휘 집합 크기를 지나치게 늘리지 않으면서도 UNK 문제를 피하고, 분석된 토큰 시퀀스 길이가 너무 길어지지 않도록 합니다. 

대표적인 서브워드 단위 토큰화 기법이 바이트 페어 인코딩(Byte Pair Encoding)인데요. 다음 장에서 살펴보겠습니다.

---
